# Workspaces

# Summary

Run persistent containers for users with web frontends, like VS Code and Jupyter Notebooks.

# Motivation and background

PrairieLearn currently allows code editing using an in-page ACE Editor and can compile and test code via external graders, which take student code and instructor-provided test code and execute them in an instructor-defined container, returning the test results back to the student. While excellent for testing small code snippets, this is not a very flexible environment for writing and debugging more complex programs.

This RFC proposes to give students persistent remote containers (called _Workspaces_) to work in, configured by instructors to provide a per-question environment with a specific set of compilers, debuggers, editors, etc. This remote container would be accessed via a web-based frontend, such as VS Code or Jupyter Notebooks.

Goals:
* Workspaces should be instructor-defined on a per-question basis.
* Workspaces should launch from a PL question with a set of initial files in the home directory, which may be dynamically generated by the question `server.py` code.
    * Workspaces should retain a readonly copy of these initial files to allow the student to reference/restore the initial state.
* The workspace should be accessible by the student via a web frontend (e.g., VS Code) that is served out of the workspace container.
* The student should have complete freedom to use the workspace as a development environment, including compiling and executing code.
* The files in the container home directory should be frequently autosaved to a persistent store, in case of container crashes (e.g., fork bombs).
* At any time, the student should be able to trigger a "Grade" action, which will pull an instructor-defined set of files from the container and run the usual PL grading code (either internal or external graders).
* Workspaces should auto-terminate after a period of inactivity (or force-terminated by the student), but should be able to be re-launched with the persistent home-directory files restored.

# Proposed solution

The server architecture has three conceptual components:
1. Main PL web servers: render questions for the student with a "launch workspace" button in them.
2. Manager servers: coordinate the launching of workspaces and proxy all traffic from the student browser through to the host machines.
3. Host servers: run the actual containers.

These three components are implemented within the main PL executable, but for deployment we can run different fleets of servers that use a `config` option to only turn on specific functionality.

## How this works

* When we want to launch a container, we bundle files, upload to S3, download to worker, spin up a container with files mounted to a known good location.
* Run filesystem watcher on mounted directory.
* When files change, we’ll do two things:
    * We’ll upload the workspace state to S3.
    * We’ll push the submission state into the database (somewhere, tbd) and store errors too?

## Frontend

* We’ll serve the page as two parts:
    * An “outer part” that PrairieLearn controls - this gives us a place to show save status, and potentially show a grade button and immediate feedback in the long run. Can also show “I’ve bricked my container, pls help” button.
    * An “inner part”, which is the page served by the container.

## Workspace container orchestration

* When we create a variant, we (maybe) create an editor session (if it’s enabled for that question). This is at this point just an entry in a database table somewhere.
* We render some kind of button to launch an editor instance for that session.
* The user clicks on that button.
* We get a request for a particular workspace instance.
* We check the authorization cookies to verify that the requesting user matches the authorized user for this workspace.
* Routes:
    * `/workspace/[uuid]` - serves outer frame
    * `/workspace/[uuid]/heartbeat`
    * `/workspace/[uuid]/container/*` - proxy `*` to inner frame
* On the host:
    * `/workspace/[uuid]/workspace/*` goes to the host that’s running this container.
    *  Within the host, we’ll proxy that to the appropriate container.
    *  Each container will probably need port 80 bound to some random, unique port that we can target for forwards.
    * The host will listen for three types of signals: launch, sync, and kill container.
* How to map requests to hosts?
    * Hosts table that stores current information about each host VM.
* How do we kill off old containers?
    * Containers are killed after either:
        * We don’t receive N heartbeats in a row.
        * The user hasn’t saved for X amount of time.

Need to make sure that cookies are inaccessible to client-side code (https://github.com/PrairieLearn/PrairieLearn/issues/2503) and on the server (we need to configure our proxy to strip out at least the Cookie header, if not more things).

## Design

### Database

* `questions`
  * Add a new `workspace_image` column
* `workspace_hosts`:
  * `id`: a unique ID for this host
  * `instance_id`: the AWS instance ID for this host
  * `hostname`: the hostname (IP address, DNS address, etc) for this host
* `workspaces`: new tables
  * `variant_id`: Question variant we're associated with
  * `id`: a unique ID for this workspace
  * `s3_bucket`: The S3 bucket that this workspace's state lives in
  * `s3_root_key`: The root "path" within the S3 bucket
  * `s3_initialized`: Whether we've created the resources for this workspace in S3 or not
  * `workspace_host_id` (nullable): The ID of the host that this workspace container is running on, if any
  * `state`: reflects the "state" of this workspace; one of the following:
    * `uninitialized`: no resources have been created for this workspace yet; can transition to `initializing`
    * `initializing`: we're creating S3 resources for this workspace; can transition to `stopped`
    * `stopped`: the workspace exists but is not running on a particular machine; can transition to `launching`
    * `launching`: we are allocating a host for this workspace and starting the appropriate container on that host; can transition to `running` or `stopped` (if launching fails)
    * `running`: the container for this workspace is running; can transition to `stopped`

### Questions

Course staff will declare workspace config per question via `workspace` in `info.json`. To begin, the only option will be an image:

```json
{
    "workspace": {
        "image": "some-docker-image"
    }
}
```

The home directory in the workspace will be determined by the `workspace` directory inside a question directory. In the future, we'll add the ability to dynamically generate files via `server.py` and place them into the home directory. This is not part of the MVP.

The workspace image will need to be synced to the `questions` table via the usual syncing code. We should use a new `workspace_image` column.

### Student-facing question interface

> What happens when we render a question with an associated workspace?

When a new variant of a question is created, we'll create a corresponding workspace in the database associated with that particular variant. This database entry will contain a unique hash/id/something. However, we're not going to actually provision any containers, etc. for this workspace just yet.

We'll introduce a new `<pl-workspace>` element that renders (to start with) a "Launch workspace" button. We should introduce a new `workspace_url` to `data.options`, and this element (or potentially other elements) can use this to render a button. `workspace_url` will be something of the form `/pl/[garbage]/workspace/[workspace_id]`, where `[garbage]` corresponds to the different places that questions can be accessed from (instructor question, student variant, maybe others?).

When this button is clicked, the URL at `workspace_url` will be opened in a new tab.

### Accessing a workspace

> What happens when a user lands on a `workspace_url`?

`workspace_url` pages will be served by the main PrairieLearn server (someday, we could split this into a separate autoscaled component).

When we get a request to this url, we'll first check if we have an existing instance of a workspace by checking the `workspace_host_id` column. There will be two cases here.

#### A container has already been allocated on a host

In this case, we can just proxy traffic directly to the appropriate host.

#### A container has not been allocated on a host

First, we'll see if we've already initialized S3 resources for this workspace in the past. If not, we'll create two files on S3:

* The initial state of the workspace
* The "working" state of the workspace that will be modified as students interact with the workspace

These two will be identical initially and will diverege over time.

Once we have these two S3 resources, we'll immediately respond with the "outer frame" HTML. At the same time, we'll allocate a host for this particular workspace and instruct it to begin loading the appropriate resources (Docker image, S3 resources, etc.).

The "outer frame" will initially render a loading screen and set up a websocket connection to the main PL server. When the workspace host has finished initializing the workspace container, we'll send a websocket message to the client to instruct it to load the "inner frame" in an iframe, which will be served by the container running on a host.

## Notes

* Since PrairieLearn will be serving a bunch of different roles depending on context, PrairieLearn's `server.js` should be split up so that only code needed to serve a particular role is loaded. While we're refactoring, let's just make it better, do things async, etc.
* There’s a distinction between workspace state and submission state - the former can include arbitrary files, the latter just includes whatever the question specifies.
