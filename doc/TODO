
##############################################################################
##############################################################################
Plan:

* Repository:
    - Separate platform from questions.
    - Make sure everyone can get PL running.

* API:
    - Document "tests" and "stats".
    - Remove qScores.
    - Shift test code into client rather than dynamically loading.
    - Review API for Arkitecktur question porting.
    - Think about hierarchical tests.
    - Add a solution box for questions.
    - Remove params arg to gradeAnswer() and replicate to a class.
    - Add simple test of all questions (or list of questions).
    - Related resources, both hard-coded and recommendation engine. Track click-throughs.
    - Import statistics from external sources (e.g., randexam).

* Hosted platform:
    - Add concept of "courses". How to handle "instances" of courses? How should question/user binding to courses work?
    - Shift questions and tests into DB (not actually necessary).
    - Add online editing and file management.

* Stability:
    - Server open files issue.
    - Client error reporting and connectivity monitoring.

* Extensions:
    - Score (and full JSON) export.
    - Statistics and analysis for students and instructors.
    - LTI support.

* Test formats:
    - Don't re-generate VID until question is answered.
    - Make sure recursive tests work ok.
    - Allow tests to generate VIDs so a question group can get the same VID.

##############################################################################
##############################################################################
Bugs:

Exam ordering is wrong after grading - Clicking "New attempt at this exam", then doing a question, then grading, then clicking "Exams" in the navbar, gives a list of exam attempts with the new exam at the bottom, not at the top where it should be.

Too many files open error after long-running server.

Recomputing statistics from scratch every 10 min is taking too long.

##############################################################################
##############################################################################
Enhancements:

Use cluster or subprocess to isolate question/test-running code and implement timeout().

GET /users should prefilter by uid for non-superusers

Add HW Q numbers to questions on exams post grading

server should enforce that question score is numeric in [0,1]

PrairieDraw should cope with missing image loads

Add median duration per question to Statistics

Server-side input validation

Anticorrelation by PrairieRandom overloading

Always reload question modules when in dev mode - The server should always reload question modules when running in dev mode. This can be done by deleting the appropriate keys from require.cache just before doing require() on the question module.

Better client-side question-error handling

Better server-side question-error handling

Better exact answer output - Add formatter to limit precision of the exact answer.

Improve historical activity views

Testing: question tests client-side and unit tests server-side

##############################################################################
##############################################################################
Testing: question tests client-side and unit tests server-side

We need an automated test-suite to check that everything is working after changes have been made. This should consist of at least two components:

1. **Server-side unit tests:** Check that GET and POST requests to the server return the expected results, including the correct errors for invalid requests. Each server API URL should have at least one correctness test and one error test.

2. **Client-side question tests:** Check that we can submit a question answer and get back the expected score. These tests should use [PhantomJS](http://phantomjs.org/) or [Selenium](http://docs.seleniumhq.org/) (possibly [Zombie.js](http://zombie.labnotes.org/)?).

Both types of tests can hopefully be driven from a single test framework, such as [Jasmine](http://pivotal.github.io/jasmine/) or [Mocha](http://visionmedia.github.io/mocha/).

##############################################################################
##############################################################################
Improve historical activity views

We would like to have a simple interface for the students to see which questions they have done in the past, and what happened with each question. This should consist of several parts:

### Server-side changes

The server needs to be changed to support pagination as follows.

`/submissions` should take `offset` and `limit` query parameters. The response should then include just entries in the range `[offset, offset + limit - 1]`. All counts are zero-indexed, so specifying `offset = 0` and `limit = 10` would give the first 10 entries, while `offset = 40` and `limit = 10` would give the fifth block of 10 entries.

### Activity page

The activity page should be nicely styled as a paged table view using the [bootstrap pagination component](http://getbootstrap.com/components/#pagination). The server-side changes listed above will enable each page to be fetched efficiently.

The date-time objects in the activity report are in [ISO 8601](http://en.wikipedia.org/wiki/ISO_8601) format in UTC. These should be displayed in the user's local timezone (see [this blog post](http://trevoro.net/2013/whats-your-timezone/)).

Each submission line on the activity page should specify:

* date/time
* question title
* score for this submission

Each submission line should link to a view of the completed question, showing the users' answer and the grading data, including score and the solution if available.

##############################################################################
##############################################################################
Better server-side question-error handling

Make sure server.js can survive and accurately report:

1. Missing server.js for questions.

2. Load/parse errors for server.js.

3. Exceptions thrown from server.js functions.

4. Invalid responses from server.js functions (extra data, missing data, mal-typed or mal-formed data (e.g., score < 0 or > 1)).

5. Errors from dependent loads via r.js.

Partially implemented in c3876b1bfa058afe65d3c23afcdc202419f69871 (at least the main server should survive question server errors).

##############################################################################
##############################################################################
Better client-side question-error handling

Make sure app.js can survive and accurately report:

* Load errors for all AJAX requests.

* Parse errors for client.js from questions.

* Exceptions thrown from question code.

* Errors and exceptions from dependent require.js loads.

##############################################################################
##############################################################################
Server-side input validation

At the moment there is almost no input validation in server.js for POST requests. We should check that all submitted data is valid before processing. This includes:

1. Check that JSON objects have the required structure, including the presence of required keys. For example, a POST to `/submissions/` must consist of an object with at least `qid`, `vid`, `qiid`, and `submittedAnswer` keys.

2. Check that the values in JSON objects have the correct data-type and format. For example, the POST data to `/submissions/` must have a value for `qid` that is firstly a string, and secondly is in fact a valid Question ID.

To check input validity there are several possible methods:

* Hand-written checks. Probably not a good idea.

* Define a [JSON schema](http://json-schema.org/) for each input object type and use a schema validator such as [json-schema](https://github.com/kriszyp/json-schema), [JaySchema](https://github.com/natesilva/jayschema), [schema.js](https://github.com/akidee/schema.js), or another from the [list of validators](http://json-schema.org/implementations.html).

* Other validators, such as [express-validator](https://github.com/ctavan/express-validator) or another from [this list](https://npmjs.org/browse/keyword/validator).

My initial guess is that the JSON schema approaches look good, combined with custom checks (for example, that `qid` is actually a Question ID).

##############################################################################
##############################################################################
Too many files open error after long-running server.

The error appears as stdout logs from the main `server.js` in the form:

    Error: EMFILE, open 'tests/tangNorm/testOverview.html'

This seems to be caused by having too many open files?

Perhaps this is due to persistent TCP connections from dead clients. E.g., `netstat` and `lsof -i` show old TCP connections still in `ESTABLISHED` state, but actually dead (due to client death, network failure, etc.).

The default TCP keepalive settings on Linux are:

    [root@prairielearn2 ~]# cat /proc/sys/net/ipv4/tcp_keepalive_time 
    7200
    [root@prairielearn2 ~]# cat /proc/sys/net/ipv4/tcp_keepalive_intvl 
    75
    [root@prairielearn2 ~]# cat /proc/sys/net/ipv4/tcp_keepalive_probes 
    9

This means that the server will wait for 7200 s (2 h) and then will send 9 probes at intervals of 75 s before force-closing the connection. Clearly this could result in many zombie connections.

The thinking would then be that `express.js` is keeping disk files open due to `sendfile()` streaming behavior. It's not clear whether this is actually true.

The maximum number of open files is:

    [root@prairielearn2 ~]# ulimit -n
    1024

This can be increased by changing `/etc/security/limits.conf` and adding:

    * hard nofile 10000

It may also be then necessary to use `ulimit` to increase the actual limit in the shell. Also, it's possible that setting a soft limit in `limits.conf` will have the same effect (need to set both hard and soft?). It may also be necessary to edit `/etc/pam.d/common-session` and add:

    session required pam_limits.so

If we want to fix the zombie TCP connection issue in Node then we could possibly use `socket.setTimeout()` or `socket.setKeepAlive()`.

Note that sockets and files are both file descriptors, and ulimit only modifies the file descriptor limit.

To replicate on OS X, open two shells, one for the server and one for the client.

1. In the server shell, decrease the max number of open files:

        ulimit -n 210

2. Start the server with this lower ulimit.

3. In the client shell, increase the max number of open files (not strictly needed here):

        launchctl limit maxfiles 1024 10000
        ulimit -n 1024

4. Start the `loadtest.js` client, with it set to do 180 connectOpen() calls (just open connections, don't write to them or close them), and `tryTransaction()` doing only `appLoad()` with:

        MAX_ACTIVE_REQS = 30
        DELAY = 10

On OS X, apparently we can also do:

* Check open file limits:

        $ sysctl -a | grep files
        kern.maxfiles = 12288
        kern.maxfilesperproc = 10240

* Increase open file limits:

        $ sudo sysctl -w kern.maxfiles=12288
        $ sudo sysctl -w kern.maxfilesperproc=10240

* Increase ulimit:

        $ ulimit -n 10240

* Check open socket limits:

        $ sysctl -a | grep somax
        kern.ipc.somaxconn: 2048

* Increase open socket limits:

        $ sudo sysctl -w kern.ipc.somaxconn=2048

From: http://b.oldhu.com/2012/07/19/increase-tcp-max-connections-on-mac-os-x/

It max also be necessary to use the `-S` flag for `ulimit`, to set the soft limit:

    ulimit -S -n 1024

To permanently change limits on OS X, edit (or create) `/etc/launchd.conf` to include the line:

    maxfiles    16384          32768

Also edit (or create) `/etc/sysctl.conf` to include:

    kern.maxfiles=20480

Apparently it is the case that on OS X you need to do all three of:

1. Increase kernel limits with `sysctl`

2. Increase launchd limits with `launchctl`

3. Increase ulimit limits with `unlimit -n`

ulimit applies only to the current shell, they other two are global.

It's not clear what order 1 and 2 need to be done in, or whether it makes a difference.

##############################################################################
##############################################################################
Old TODOs:

- write out per-question alpha,beta,gamma histories and plot them

- write out per-user sigma histories and plot them

- investigate why 2D doesn't help in prediction accuracy

- on stats webpage, make per-question plots showing PDFs for each
  question. The independent PDF variable should be users, ranked by
  predicted prob on this question. Should include all users, not just
  those who have tried the question. This is basically a different way
  of viewing the color bars, for predicted data.

- add "table-striped table-hover" to homework panel tables once
  Bootstrap v3.1 is released:
  https://github.com/twbs/bootstrap/issues/10492

- Questions:

  -- Can we use unscented-type quad points with only two points per
     dim?

  -- Can we extract any useful information from question-choice
     ordering by students?

  -- Can we use timing data in some useful way? E.g., time to complete
     questions successfully, time for failed attempts, etc.

  -- What is an explicit state model for spaced-repetition learning?
     How do we include the actual time delay effects on memory? See
     what the literature has to say about models for this, starting on
     Gwern's site?

##############################################################################
##############################################################################
